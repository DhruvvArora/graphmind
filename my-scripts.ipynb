{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def greeting_tool() -> str:\n",
    "    \"\"\"Fetches a greeting message from OpenAI.\"\"\"\n",
    "    response = llm.invoke({\"prompt\": \"Provide a friendly greeting message.\"})\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "@tool\n",
    "def farewell_tool() -> str:\n",
    "    \"\"\"Fetches a farewell message from OpenAI.\"\"\"\n",
    "    response = llm.invoke({\"prompt\": \"Provide a polite farewell message.\"})\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "@tool\n",
    "def medicine_tool() -> str:\n",
    "    \"\"\"Fetches information about recommended medicines for common symptoms like fever or headache.\"\"\"\n",
    "    response = llm.invoke({\"prompt\": \"Provide a list of recommended medicines for treating common symptoms such as fever or headache.\"})\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "@tool\n",
    "def medical_hospital_tool() -> str:\n",
    "    \"\"\"Fetches information about nearby hospitals based on general location data.\"\"\"\n",
    "    response = llm.invoke({\"prompt\": \"Provide information about nearby hospitals for general medical assistance.\"})\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "@tool\n",
    "def medical_department_tool() -> str:\n",
    "    \"\"\"Fetches information about medical departments available in a hospital.\"\"\"\n",
    "    response = llm.invoke({\"prompt\": \"List common medical departments in a hospital and their primary services.\"})\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph \\ Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a medical assistant providing the information related to the medical field.\"\n",
    "    \"You are also a supervisor tasked with managing a conversation between the following agents: {members}. \"\n",
    "    \"Each agent provides specific information related to its expertise area in response to user queries. \"\n",
    "    \"For example, MedicineAgent should give medical advice within general advice limitations, MedicalHospitalAgent should suggest hospitals based on location, and MedicalDepartmentAgent should offer relevant departmental options.\"\n",
    ")\n",
    "\n",
    "members = [\"GreetingAgent\", \"FarewellAgent\", \"MedicineAgent\", \"MedicalHospitalAgent\", \"MedicalDepartmentAgent\"]\n",
    "\n",
    "class routeResponse(BaseModel):\n",
    "    next: Literal[\"FINISH\", \"GreetingAgent\", \"FarewellAgent\", \"MedicineAgent\", \"MedicalHospitalAgent\", \"MedicalDepartmentAgent\"]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next? \"\n",
    "            \"Or should we FINISH? Select one of: ['GreetingAgent', 'FarewellAgent', 'MedicineAgent', 'MedicalHospitalAgent', 'MedicalDepartmentAgent', 'FINISH']\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(members=\", \".join(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    if \"messages\" in result and result[\"messages\"]:\n",
    "        last_message = result[\"messages\"][-1]\n",
    "        message_content = last_message.content\n",
    "        return {\n",
    "            \"messages\": [HumanMessage(content=message_content, name=name)]\n",
    "        }\n",
    "    else:\n",
    "        print(f\"{name} agent did not return a message.\")\n",
    "        return {\"messages\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Proxy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserProxyAgent(state):\n",
    "    return {\"messages\": state[\"messages\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator (Supervisor) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OperatorAgent(state):\n",
    "    supervisor_chain = prompt | llm.with_structured_output(routeResponse)\n",
    "    response = supervisor_chain.invoke(state)\n",
    "    \n",
    "    if hasattr(response, 'next'):\n",
    "        print(f\"Operator selected: {response.next}\")\n",
    "        return {\"next\": response.next, \"messages\": state.get(\"messages\", [])}\n",
    "    else:\n",
    "        print(\"Error: 'next' not found in operator response.\")\n",
    "        return {\"next\": \"FINISH\", \"messages\": state.get(\"messages\", [])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialized Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "import functools\n",
    "\n",
    "GreetingsAgent = create_react_agent(llm, tools=[greeting_tool])\n",
    "GreetingsNode = functools.partial(agent_node, agent=GreetingsAgent, name=\"GreetingAgent\")\n",
    "\n",
    "FarewellAgent = create_react_agent(llm, tools=[farewell_tool])\n",
    "FarewellNode = functools.partial(agent_node, agent=FarewellAgent, name=\"FarewellAgent\")\n",
    "\n",
    "MedicineAgent = create_react_agent(llm, tools=[medicine_tool])\n",
    "MedicineNode = functools.partial(agent_node, agent=MedicineAgent, name=\"MedicineAgent\")\n",
    "\n",
    "MedicalHospitalAgent = create_react_agent(llm, tools=[medical_hospital_tool])\n",
    "MedicalHospitalNode = functools.partial(agent_node, agent=MedicalHospitalAgent, name=\"MedicalHospitalAgent\")\n",
    "\n",
    "MedicalDepartmentAgent = create_react_agent(llm, tools=[medical_department_tool])\n",
    "MedicalDepartmentNode = functools.partial(agent_node, agent=MedicalDepartmentAgent, name=\"MedicalDepartmentAgent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"UserProxy\", UserProxyAgent)\n",
    "workflow.add_node(\"GreetingAgent\", GreetingsNode)\n",
    "workflow.add_node(\"FarewellAgent\", FarewellNode)\n",
    "workflow.add_node(\"Operator\", OperatorAgent)\n",
    "workflow.add_node(\"MedicineAgent\", MedicineNode)\n",
    "workflow.add_node(\"MedicalHospitalAgent\", MedicalHospitalNode)\n",
    "workflow.add_node(\"MedicalDepartmentAgent\", MedicalDepartmentNode)\n",
    "\n",
    "conditional_map = {\n",
    "    \"GreetingAgent\": \"GreetingAgent\",\n",
    "    \"FarewellAgent\": \"FarewellAgent\",\n",
    "    \"MedicineAgent\": \"MedicineAgent\",\n",
    "    \"MedicalHospitalAgent\": \"MedicalHospitalAgent\",\n",
    "    \"MedicalDepartmentAgent\": \"MedicalDepartmentAgent\",\n",
    "    \"FINISH\": END\n",
    "}\n",
    "workflow.add_conditional_edges(\"Operator\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "workflow.add_edge(START, \"UserProxy\")\n",
    "workflow.add_edge(\"UserProxy\", \"Operator\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I am a Medical Chatbot configured by LangGraph. (type 'exit' to end with assistance)\")\n",
    "user_messages = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting the chatbot. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    user_messages.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    for state in graph.stream({\"messages\": user_messages}):\n",
    "        if \"__end__\" not in state:\n",
    "            for agent_name, agent_response in state.items():\n",
    "                if \"messages\" in agent_response:\n",
    "                    message_content = agent_response[\"messages\"][-1].content\n",
    "                    print(f\"{agent_name}: {message_content}\")\n",
    "                    user_messages.append(agent_response[\"messages\"][-1])\n",
    "                else:\n",
    "                    print(f\"No message content available in response from {agent_name}.\")\n",
    "            print(\"--------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
